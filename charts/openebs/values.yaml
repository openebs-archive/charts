# Default values for openebs.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

rbac:
  # Specifies whether RBAC resources should be created
  create: true
  pspEnabled: false
  # rbac.kyvernoEnabled: `true` if Kyverno Policy resources should be created
  kyvernoEnabled: false

serviceAccount:
  create: true
  name:

imagePullSecrets: []
  #  - name: image-pull-secret

release:
  # "openebs.io/version" label for control plane components
  version: "3.4.0"

# Legacy components will be installed if it is enabled.
# Legacy components are - admission-server, maya api-server, snapshot-operator
# and k8s-provisioner
legacy:
  enabled: false

image:
  pullPolicy: IfNotPresent
  repository: ""

apiserver:
  enabled: true
  image: "openebs/m-apiserver"
  imageTag: "2.12.2"
  replicas: 1
  ports:
    externalPort: 5656
    internalPort: 5656
  sparse:
    enabled: "false"
  nodeSelector: {}
  tolerations: []
  affinity: {}
  healthCheck:
    initialDelaySeconds: 30
    periodSeconds: 60
  ## apiserver resource requests and limits
  ## Reference: http://kubernetes.io/docs/user-guide/compute-resources/
  resources: {}
    # limits:
    #   cpu: 1000m
    #   memory: 2Gi
    # requests:
    #   cpu: 500m
    #   memory: 1Gi


defaultStorageConfig:
  enabled: "true"

# Directory used by the OpenEBS to store debug information and so forth
# that are generated in the course of running OpenEBS containers.
varDirectoryPath:
  baseDir: "/var/openebs"

provisioner:
  enabled: true
  image: "openebs/openebs-k8s-provisioner"
  imageTag: "2.12.2"
  replicas: 1
  enableLeaderElection: true
  patchJivaNodeAffinity: enabled
  nodeSelector: {}
  tolerations: []
  affinity: {}
  healthCheck:
    initialDelaySeconds: 30
    periodSeconds: 60
  ## provisioner resource requests and limits
  ## Reference: http://kubernetes.io/docs/user-guide/compute-resources/
  resources: {}
    # limits:
    #   cpu: 1000m
    #   memory: 2Gi
    # requests:
    #   cpu: 500m
    #   memory: 1Gi

# If you want to enable local pv as a dependency chart then set
# `localprovisioner.enabled: false` and enable it as dependency chart.
# If you are using custom configuration then update those configuration
# under `localpv-provisioner` key.
localprovisioner:
  enabled: true
  image: "openebs/provisioner-localpv"
  imageTag: "3.4.0"
  replicas: 1
  enableLeaderElection: true
  # These fields are deprecated. Please use the fields (see below)
  # - deviceClass.enabled
  # - hostpathClass.enabled
  enableDeviceClass: true
  enableHostpathClass: true
  # This sets default directory used by the provisioner to provision
  # hostpath volumes.
  basePath: "/var/openebs/local"
  # This sets the number of times the provisioner should try
  # with a polling interval of 5 seconds, to get the Blockdevice
  # Name from a BlockDeviceClaim, before the BlockDeviceClaim
  # is deleted. E.g. 12 * 5 seconds = 60 seconds timeout
  waitForBDBindTimeoutRetryCount: "12"
  nodeSelector: {}
  tolerations: []
  affinity: {}
  healthCheck:
    initialDelaySeconds: 30
    periodSeconds: 60
  ## localprovisioner resource requests and limits
  ## Reference: http://kubernetes.io/docs/user-guide/compute-resources/
  resources: {}
    # limits:
    #   cpu: 1000m
    #   memory: 2Gi
    # requests:
    #   cpu: 500m
    #   memory: 1Gi

  deviceClass:
    # Name of default device StorageClass.
    name: openebs-device
    # If true, enables creation of the openebs-device StorageClass
    enabled: true
    # Available reclaim policies: Delete/Retain, defaults: Delete.
    reclaimPolicy: Delete
    # If true, sets the openebs-device StorageClass as the default StorageClass
    isDefaultClass: false
    # Custom node affinity label(s) for example "openebs.io/node-affinity-value"
    # that will be used instead of hostnames
    # This helps in cases where the hostname changes when the node is removed and
    # added back with the disks still intact.
    # Example:
    #          nodeAffinityLabels:
    #            - "openebs.io/node-affinity-key-1"
    #            - "openebs.io/node-affinity-key-2"
    nodeAffinityLabels: []
    # Sets the filesystem to be written to the blockdevice before
    # mounting (filesystem volumes)
    # This is only usable if the selected BlockDevice does not already
    # have a filesystem
    # Valid values: "ext4", "xfs"
    fsType: "ext4"
    # Label block devices in the cluster that you would like the openEBS localPV
    # Provisioner to pick up those specific block devices available on the node.
    # Set the label key and value as shown in the example below.
    #
    # To read more: https://github.com/openebs/dynamic-localpv-provisioner/blob/develop/docs/tutorials/device/blockdevicetag.md
    #
    # Example:
    #          blockDeviceSelectors:
    #            ndm.io/driveType: "SSD"
    #            ndm.io/fsType: "none"
    blockDeviceSelectors: {}

  hostpathClass:
    # Name of the default hostpath StorageClass
    name: openebs-hostpath
    # If true, enables creation of the openebs-hostpath StorageClass
    enabled: true
    # Available reclaim policies: Delete/Retain, defaults: Delete.
    reclaimPolicy: Delete
    # If true, sets the openebs-hostpath StorageClass as the default StorageClass
    isDefaultClass: false
    # Path on the host where local volumes of this storage class are mounted under.
    # NOTE: If not specified, this defaults to the value of localprovisioner.basePath.
    basePath: ""
    # Custom node affinity label(s) for example "openebs.io/node-affinity-value"
    # that will be used instead of hostnames
    # This helps in cases where the hostname changes when the node is removed and
    # added back with the disks still intact.
    # Example:
    #          nodeAffinityLabels:
    #            - "openebs.io/node-affinity-key-1"
    #            - "openebs.io/node-affinity-key-2"
    nodeAffinityLabels: []
    # Prerequisite: XFS Quota requires an XFS filesystem mounted with
    # the 'pquota' or 'prjquota' mount option.
    xfsQuota:
      # If true, enables XFS project quota
      enabled: false
      # Detailed configuration options for XFS project quota.
      # If XFS Quota is enabled with the default values, the usage limit
      # is set at the storage capacity specified in the PVC.
      softLimitGrace: "0%"
      hardLimitGrace: "0%"
    # Prerequisite: EXT4 Quota requires an EXT4 filesystem mounted with
    # the 'prjquota' mount option.
    ext4Quota:
      # If true, enables XFS project quota
      enabled: false
      # Detailed configuration options for EXT4 project quota.
      # If EXT4 Quota is enabled with the default values, the usage limit
      # is set at the storage capacity specified in the PVC.
      softLimitGrace: "0%"
      hardLimitGrace: "0%"

snapshotOperator:
  enabled: true
  controller:
    image: "openebs/snapshot-controller"
    imageTag: "2.12.2"
    ## snapshot controller resource requests and limits
    ## Reference: http://kubernetes.io/docs/user-guide/compute-resources/
    resources: {}
      # limits:
      #   cpu: 1000m
      #   memory: 2Gi
      # requests:
      #   cpu: 500m
      #   memory: 1Gi
  provisioner:
    image: "openebs/snapshot-provisioner"
    imageTag: "2.12.2"
    ## snapshot provisioner resource requests and limits
    ## Reference: http://kubernetes.io/docs/user-guide/compute-resources/
    resources: {}
      # limits:
      #   cpu: 1000m
      #   memory: 2Gi
      # requests:
      #   cpu: 500m
      #   memory: 1Gi
  replicas: 1
  enableLeaderElection: true
  upgradeStrategy: "Recreate"
  nodeSelector: {}
  tolerations: []
  affinity: {}
  healthCheck:
    initialDelaySeconds: 30
    periodSeconds: 60

# If you want to enable openebs as a dependency chart then set `ndm.enabled: false`,
# `ndmOperator.enabled: false` and enable it as dependency chart. If you are using
# custom configuration then update those configuration under `openebs-ndm` key.
ndm:
  enabled: true
  image: "openebs/node-disk-manager"
  imageTag: "2.1.0"
  sparse:
    path: "/var/openebs/sparse"
    size: "10737418240"
    count: "0"
  filters:
    enableOsDiskExcludeFilter: true
    osDiskExcludePaths: "/,/etc/hosts,/boot"
    enableVendorFilter: true
    excludeVendors: "CLOUDBYT,OpenEBS"
    enablePathFilter: true
    includePaths: ""
    excludePaths: "/dev/loop,/dev/fd0,/dev/sr0,/dev/ram,/dev/dm-,/dev/md,/dev/rbd,/dev/zd"
  probes:
    enableSeachest: false
  nodeSelector: {}
  tolerations: []
  healthCheck:
    initialDelaySeconds: 30
    periodSeconds: 60
  ## ndm resource requests and limits
  ## Reference: http://kubernetes.io/docs/user-guide/compute-resources/
  resources: {}
    # limits:
    #   cpu: 1000m
    #   memory: 2Gi
    # requests:
    #   cpu: 500m
    #   memory: 1Gi

# If you want to enable openebs as a dependency chart then set `ndm.enabled: false`,
# `ndmOperator.enabled: false` and enable it as dependency chart. If you are using
# custom configuration then update those configuration under `openebs-ndm` key.
ndmOperator:
  enabled: true
  image: "openebs/node-disk-operator"
  imageTag: "2.1.0"
  replicas: 1
  upgradeStrategy: Recreate
  nodeSelector: {}
  tolerations: []
  healthCheck:
    initialDelaySeconds: 15
    periodSeconds: 20
  readinessCheck:
    initialDelaySeconds: 5
    periodSeconds: 10
  ## ndmOperator resource requests and limits
  ## Reference: http://kubernetes.io/docs/user-guide/compute-resources/
  resources: {}
    # limits:
    #   cpu: 1000m
    #   memory: 2Gi
    # requests:
    #   cpu: 500m
    #   memory: 1Gi

ndmExporter:
  enabled: false
  image:
    registry:
    repository: openebs/node-disk-exporter
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: 2.1.0
  nodeExporter:
    name: ndm-node-exporter
    podLabels:
      name: openebs-ndm-node-exporter
    # The TCP port number used for exposing ndm-node-exporter metrics.
    # If not set, service will not be created to expose metrics endpoint to serviceMonitor
    # and listen-port flag will not be set and container port will be empty.
    metricsPort: 9101
  clusterExporter:
    name: ndm-cluster-exporter
    podLabels:
      name: openebs-ndm-cluster-exporter
    # The TCP port number used for exposing ndm-cluster-exporter metrics.
    # If not set, service will not be created to expose metrics endpoint to serviceMonitor
    # and listen-port flag will not be set and container port will be empty.
    metricsPort: 9100

webhook:
  enabled: true
  image: "openebs/admission-server"
  imageTag: "2.12.2"
  failurePolicy: "Fail"
  replicas: 1
  healthCheck:
    initialDelaySeconds: 30
    periodSeconds: 60
  nodeSelector: {}
  tolerations: []
  affinity: {}
  hostNetwork: false
  ## admission-server resource requests and limits
  ## Reference: http://kubernetes.io/docs/user-guide/compute-resources/
  resources: {}
    # limits:
    #   cpu: 500m
    #   memory: 1Gi
    # requests:
    #   cpu: 250m
    #   memory: 500Mi

# If you are migrating from 2.x to 3.x and if you are using custom values
# then put this configuration under `localpv-provisioner` and `openebs-ndm` key.
helper:
  image: "openebs/linux-utils"
  imageTag: "3.4.0"

# These are ndm related configuration. If you want to enable openebs as a dependency
# chart then set `ndm.enabled: false`, `ndmOperator.enabled: false` and enable it as
# dependency chart. If you are using custom configuration then update those configuration
# under `openebs-ndm` key.
featureGates:
  enabled: true
  GPTBasedUUID:
    enabled: true
    featureGateFlag: "GPTBasedUUID"
  APIService:
    enabled: false
    featureGateFlag: "APIService"
    address: "0.0.0.0:9115"
  UseOSDisk:
    enabled: false
    featureGateFlag: "UseOSDisk"
  ChangeDetection:
    enabled: false
    featureGateFlag: "ChangeDetection"
  PartitionTableUUID:
    enabled: false
    featureGateFlag: "PartitionTableUUID"

crd:
  enableInstall: true

# If you are migrating from 2.x to 3.x and if you are using custom values
# then put these configuration under `cstor` key.
policies:
  monitoring:
    enabled: true
    image: "openebs/m-exporter"
    imageTag: "2.12.2"

analytics:
  enabled: true
  # Specify in hours the duration after which a ping event needs to be sent.
  pingInterval: "24h"

mayastor:
  enabled: false
  image:
  # -- Image registry to pull our product images
  registry: docker.io
  # -- Image registry's namespace
  repo: openebs
  # -- Release tag for our images
  tag: develop
  # -- ImagePullPolicy for our images
  pullPolicy: Always

# -- Node labels for pod assignment
# ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
# Note that if multi-arch images support 'kubernetes.io/arch: amd64'
# should be removed and set 'nodeSelector' to empty '{}' as default value.
nodeSelector:
  kubernetes.io/arch: amd64

earlyEvictionTolerations:
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 5
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 5

base:
  # -- Request timeout for rest & core agents
  default_req_timeout: 5s
  # -- Cache timeout for core agent & diskpool deployment
  cache_poll_period: 30s
  # -- Silence specific module components
  logSilenceLevel:
  initContainers:
    enabled: true
    containers:
      - name: agent-core-grpc-probe
        image: busybox:latest
        command: ['sh', '-c', 'trap "exit 1" TERM; until nc -vzw 5 {{ .Release.Name }}-agent-core 50051; do date; echo "Waiting for agent-core-grpc services..."; sleep 1; done;']
      - name: etcd-probe
        image: busybox:latest
        command: ['sh', '-c', 'trap "exit 1" TERM; until nc -vzw 5 {{ .Release.Name }}-etcd {{ .Values.etcd.service.port }}; do date; echo "Waiting for etcd..."; sleep 1; done;']
  initHaNodeContainers:
    enabled: true
    containers:
      - name: agent-cluster-grpc-probe
        image: busybox:latest
        command: ['sh', '-c', 'trap "exit 1" TERM; until nc -vzw 5 {{ .Release.Name }}-agent-core 50052; do date; echo "Waiting for agent-cluster-grpc services..."; sleep 1; done;']
  initCoreContainers:
    enabled: true
    containers:
      - name: etcd-probe
        image: busybox:latest
        command: ['sh', '-c', 'trap "exit 1" TERM; until nc -vzw 5 {{ .Release.Name }}-etcd {{ .Values.etcd.service.port }}; do date; echo "Waiting for etcd..."; sleep 1; done;']
  # docker-secrets required to pull images if the container registry from image.Registry is protected
  imagePullSecrets:
    # -- Enable imagePullSecrets for pulling our container images
    enabled: false
    # Name of the imagePullSecret in the installed namespace
    secrets:
      - name: login

  metrics:
    # -- Enable the metrics exporter
    enabled: true
    # metrics refresh time
    # WARNING: Lowering pollingInterval value will affect performance adversely
    pollingInterval: "5m"

  jaeger:
    # -- Enable jaeger tracing
    enabled: false
    initContainer: true
    agent:
      name: jaeger-agent
      port: 6831
      initContainer:
        - name: jaeger-probe
          image: busybox:latest
          command: ['sh', '-c', 'trap "exit 1" TERM; until nc -vzw 5 -u {{.Values.base.jaeger.agent.name}} {{.Values.base.jaeger.agent.port}}; do date; echo "Waiting for jaeger..."; sleep 1; done;']
  initRestContainer:
    enabled: true
    initContainer:
      - name: api-rest-probe
        image: busybox:latest
        command: ['sh', '-c', 'trap "exit 1" TERM; until nc -vzw 5 {{ .Release.Name }}-api-rest 8081; do date; echo "Waiting for REST API endpoint to become available"; sleep 1; done;']

operators:
  pool:
    # -- Log level for diskpool operator service
    logLevel: info
    resources:
      limits:
        # -- Cpu limits for diskpool operator
        cpu: "100m"
        # -- Memory limits for diskpool operator
        memory: "32Mi"
      requests:
        # -- Cpu requests for diskpool operator
        cpu: "50m"
        # -- Memory requests for diskpool operator
        memory: "16Mi"

jaeger-operator:
  # Name of jaeger operator
  name: "{{ .Release.Name }}"
  crd:
    # Install jaeger CRDs
    install: false
  jaeger:
    # Install jaeger-operator
    create: false
  rbac:
    # Create a clusterRole for Jaeger
    clusterRole: true

agents:
  core:
    # -- Log level for the core service
    logLevel: info
    resources:
      limits:
        # -- Cpu limits for core agents
        cpu: "1000m"
        # -- Memory limits for core agents
        memory: "128Mi"
      requests:
        # -- Cpu requests for core agents
        cpu: "500m"
        # -- Memory requests for core agents
        memory: "32Mi"
  ha:
    enabled: true
    node:
      # -- Log level for the ha node service
      logLevel: info
      resources:
        limits:
          # -- Cpu limits for ha node agent
          cpu: "100m"
          # -- Memory limits for ha node agent
          memory: "64Mi"
        requests:
          # -- Cpu requests for ha node agent
          cpu: "100m"
          # -- Memory requests for ha node agent
          memory: "64Mi"
    cluster:
      # -- Log level for the ha cluster service
      logLevel: info
      resources:
        limits:
          # -- Cpu limits for ha cluster agent
          cpu: "100m"
          # -- Memory limits for ha cluster agent
          memory: "64Mi"
        requests:
          # -- Cpu requests for ha cluster agent
          cpu: "100m"
          # -- Memory requests for ha cluster agent
          memory: "16Mi"

apis:
  rest:
    # -- Log level for the rest service
    logLevel: info
    # -- Number of replicas of rest
    replicaCount: 1
    resources:
      limits:
        # -- Cpu limits for rest
        cpu: "100m"
        # -- Memory limits for rest
        memory: "64Mi"
      requests:
        # -- Cpu requests for rest
        cpu: "50m"
        # -- Memory requests for rest
        memory: "32Mi"
    # Rest service parameters define how the rest service is exposed
    service:
      # -- Rest K8s service type
      type: ClusterIP
      # Ports from where rest endpoints are accessible from outside the cluster, only valid if type is NodePort
      nodePorts:
        # NodePort associated with http port
        http: 30011
        # NodePort associated with https port
        https: 30010

csi:
  image:
    # -- Image registry to pull all CSI Sidecar images
    registry: registry.k8s.io
    # -- Image registry's namespace
    repo: sig-storage
    # -- imagePullPolicy for all CSI Sidecar images
    pullPolicy: IfNotPresent
    # -- csi-provisioner image release tag
    provisionerTag: v2.2.1
    # -- csi-attacher image release tag
    attacherTag: v3.2.1
    # -- csi-node-driver-registrar image release tag
    registrarTag: v2.1.0

  controller:
    # -- Log level for the csi controller
    logLevel: info
    resources:
      limits:
        # -- Cpu limits for csi controller
        cpu: "32m"
        # -- Memory limits for csi controller
        memory: "128Mi"
      requests:
        # -- Cpu requests for csi controller
        cpu: "16m"
        # -- Memory requests for csi controller
        memory: "64Mi"
  node:
    logLevel: info
    topology:
      segments:
        openebs.io/csi-node: mayastor
      # -- Add topology segments to the csi-node daemonset node selector
      nodeSelector: false
    resources:
      limits:
        # -- Cpu limits for csi node plugin
        cpu: "100m"
        # -- Memory limits for csi node plugin
        memory: "128Mi"
      requests:
        # -- Cpu requests for csi node plugin
        cpu: "100m"
        # -- Memory requests for csi node plugin
        memory: "64Mi"
    nvme:
      # -- The nvme_core module io timeout in seconds
      io_timeout: "30"
      # -- The ctrl_loss_tmo (controller loss timeout) in seconds
      ctrl_loss_tmo: "1980"
      # Kato (keep alive timeout) in seconds
      keep_alive_tmo: ""
    # -- The kubeletDir directory for the csi-node plugin
    kubeletDir: /var/lib/kubelet
    pluginMounthPath: /csi
    socketPath: csi.sock

io_engine:
  # -- Log level for the io-engine service
  logLevel: info,io_engine=info
  api: "v1"
  target:
    nvmf:
      # -- NVMF target interface (ip, mac, name or subnet)
      iface: ""
      # -- Reservations Persist Through Power Loss State
      ptpl: true
  # -- Pass additional arguments to the Environment Abstraction Layer.
  # Example: --set {product}.envcontext=iova-mode=pa
  envcontext: ""
  reactorFreezeDetection:
    enabled: false
  # -- The number of cpu that each io-engine instance will bind to.
  cpuCount: "2"
  # -- Node selectors to designate storage nodes for diskpool creation
  # Note that if multi-arch images support 'kubernetes.io/arch: amd64'
  # should be removed.
  nodeSelector:
    openebs.io/engine: mayastor
    kubernetes.io/arch: amd64
  resources:
    limits:
      # -- Cpu limits for the io-engine
      cpu: ""
      # -- Memory limits for the io-engine
      memory: "1Gi"
      # -- Hugepage size available on the nodes
      hugepages2Mi: "2Gi"
    requests:
      # -- Cpu requests for the io-engine
      cpu: ""
      # -- Memory requests for the io-engine
      memory: "1Gi"
      # -- Hugepage size available on the nodes
      hugepages2Mi: "2Gi"

etcd:
  # Pod labels; okay to remove the openebs logging label if required
  podLabels:
    app: etcd
    openebs.io/logging: "true"
  # -- Number of replicas of etcd
  replicaCount: 3
  # Kubernetes Cluster Domain
  clusterDomain: cluster.local
  # TLS authentication for client-to-server communications
  # ref: https://etcd.io/docs/current/op-guide/security/
  client:
    secureTransport: false
  # TLS authentication for server-to-server communications
  # ref: https://etcd.io/docs/current/op-guide/security/
  peer:
    secureTransport: false
  # Enable persistence using Persistent Volume Claims
  persistence:
    # -- If true, use a Persistent Volume Claim. If false, use emptyDir.
    enabled: true
    # -- Will define which storageClass to use in etcd's StatefulSets
    # a `manual` storageClass will provision a hostpath PV on the same node
    # an empty storageClass will use the default StorageClass on the cluster
    storageClass: ""
    # -- Volume size
    size: 2Gi
    # -- PVC's reclaimPolicy
    reclaimPolicy: "Delete"
  # -- Use a PreStop hook to remove the etcd members from the etcd cluster on container termination
  # Ignored if lifecycleHooks is set or replicaCount=1
  removeMemberOnContainerTermination: false

  # -- AutoCompaction
  # Since etcd keeps an exact history of its keyspace, this history should be
  # periodically compacted to avoid performance degradation
  # and eventual storage space exhaustion.
  # Auto compaction mode. Valid values: "periodic", "revision".
  # - 'periodic' for duration based retention, defaulting to hours if no time unit is provided (e.g. 5m).
  # - 'revision' for revision number based retention.
  autoCompactionMode: revision
  # -- Auto compaction retention length. 0 means disable auto compaction.
  autoCompactionRetention: 100
  extraEnvVars:
    # -- Raise alarms when backend size exceeds the given quota.
    - name: ETCD_QUOTA_BACKEND_BYTES
      value: "8589934592"

  auth:
    rbac:
      create: false
      enabled: false
      allowNoneAuthentication: true
  # Init containers parameters:
  # volumePermissions: Change the owner and group of the persistent volume mountpoint to runAsUser:fsGroup values from the securityContext section.
  #
  volumePermissions:
    # chown the mounted volume; this is required if a statically provisioned hostpath volume is used
    enabled: true
  # extra debug information on logs
  debug: false
  initialClusterState: "new"
  # Pod anti-affinity preset
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  podAntiAffinityPreset: "hard"

  # etcd service parameters defines how the etcd service is exposed
  service:
    # K8s service type
    type: ClusterIP

    # etcd client port
    port: 2379

    # Specify the nodePort(s) value(s) for the LoadBalancer and NodePort service types.
    # ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
    #
    nodePorts:
      # Port from where etcd endpoints are accessible from outside cluster
      clientPort: 31379
      peerPort: ""

loki-stack:
  # -- Enable loki log collection for our components
  enabled: true
  loki:
    rbac:
      # -- Create rbac roles for loki
      create: true
      pspEnabled: false
    # -- Enable loki installation as part of loki-stack
    enabled: true
    # Install loki with persistence storage
    persistence:
      # -- Enable persistence storage for the logs
      enabled: true
      # -- StorageClass for Loki's centralised log storage
      # empty storageClass implies cluster default storageClass & `manual` creates a static hostpath PV
      storageClassName: ""
      # -- PVC's ReclaimPolicy, can be Delete or Retain
      reclaimPolicy: "Delete"
      # -- Size of Loki's persistence storage
      size: 10Gi
    # loki process run & file permissions, required if sc=manual
    securityContext:
      fsGroup: 1001
      runAsGroup: 1001
      runAsNonRoot: false
      runAsUser: 1001
    # initContainers to chown the static hostpath PV by 1001 user
    initContainers:
      - command: ["/bin/bash", "-ec", "chown -R 1001:1001 /data"]
        image: docker.io/bitnami/bitnami-shell:10
        imagePullPolicy: IfNotPresent
        name: volume-permissions
        securityContext:
          runAsUser: 0
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
          - mountPath: /data
            name: storage
    config:
      # Compactor is a BoltDB(loki term) Shipper specific service that reduces the index
      # size by deduping the index and merging all the files to a single file per table.
      # Ref: https://grafana.com/docs/loki/latest/operations/storage/retention/
      compactor:
        # Dictates how often compaction and/or retention is applied. If the
        # Compactor falls behind, compaction and/or retention occur as soon as possible.
        compaction_interval: 20m

        # If not enabled compactor will only compact table but they will not get
        # deleted
        retention_enabled: true

        # The delay after which the compactor will delete marked chunks
        retention_delete_delay: 1h

        # Specifies the maximum quantity of goroutine workers instantiated to
        # delete chunks
        retention_delete_worker_count: 50

      # Rentention period of logs is configured within the limits_config section
      limits_config:
        # configuring retention period for logs
        retention_period: 168h

    # Loki service parameters defines how the Loki service is exposed
    service:
      # K8s service type
      type: ClusterIP
      port: 3100
      # Port where REST endpoints of Loki are accessible from outside cluster
      nodePort: 31001

  # promtail configuration
  promtail:
    rbac:
      # create rbac roles for promtail
      create: true
      pspEnabled: false
    # -- Enables promtail for scraping logs from nodes
    enabled: true
    # -- Disallow promtail from running on the master node
    tolerations: []
    config:
      # -- The Loki address to post logs to
      lokiAddress: http://{{ .Release.Name }}-loki:3100/loki/api/v1/push
      snippets:
        # Promtail will export logs to loki only based on based on below
        # configuration, below scrape config will export only our services
        # which are labeled with openebs.io/logging=true
        scrapeConfigs: |
          - job_name: {{ .Release.Name }}-pods-name
            pipeline_stages:
              - docker: {}
            kubernetes_sd_configs:
            - role: pod
            relabel_configs:
            - source_labels:
              - __meta_kubernetes_pod_node_name
              target_label: hostname
              action: replace
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - action: keep
              source_labels:
              - __meta_kubernetes_pod_label_openebs_io_logging
              regex: true
              target_label: {{ .Release.Name }}_component
            - action: replace
              replacement: $1
              separator: /
              source_labels:
              - __meta_kubernetes_namespace
              target_label: job
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_name
              target_label: pod
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_container_name
              target_label: container
            - replacement: /var/log/pods/*$1/*.log
              separator: /
              source_labels:
              - __meta_kubernetes_pod_uid
              - __meta_kubernetes_pod_container_name
              target_label: __path__
obs:
  callhome:
    # -- Enable callhome
    enabled: true
    # -- Log level for callhome
    logLevel: "info"
    resources:
      limits:
        # -- Cpu limits for callhome
        cpu: "100m"
        # -- Memory limits for callhome
        memory: "32Mi"
      requests:
        # -- Cpu requests for callhome
        cpu: "50m"
        # -- Memory requests for callhome
        memory: "16Mi"   

jiva:

  # non csi configuration
  image: "openebs/jiva"
  imageTag: "2.12.2"
  replicas: 3
  defaultStoragePath: "/var/openebs"

  # jiva csi driver configuration
  # do not enable or configure any sub dependency here
  # only jiva csi related settings can be added here
  # ref - https://openebs.github.io/jiva-operator

  # jiva chart dependency tree is here -
  # jiva
  # | - localpv-provisioner
  # | | - openebs-ndm

  # Enable localpv-provisioner and openebs-ndm as root dependency not as
  # sub dependency.
  # openebs
  # | - jiva(enable)
  # | | - localpv-provisioner(disable)
  # | | | - openebs-ndm(disable)
  # | - localpv-provisioner(enable)
  # | - openebs-ndm(enable)

  enabled: false
  openebsLocalpv:
    enabled: false
  localpv-provisioner:
    openebsNDM:
      enabled: false

  # Sample configuration if you want to configure jiva csi driver with custom values.
  # This is a small part of the full configuration. Full configuration available
  # here - https://openebs.github.io/jiva-operator

#  rbac:
#    create: true
#    pspEnabled: false
#
#  jivaOperator:
#    controller:
#      image:
#        registry: quay.io/
#        repository: openebs/jiva
#        tag: 3.4.0
#    replica:
#      image:
#        registry: quay.io/
#        repository: openebs/jiva
#        tag: 3.4.0
#    image:
#      registry: quay.io/
#      repository: openebs/jiva-operator
#      pullPolicy: IfNotPresent
#      tag: 3.4.0
#
#  jivaCSIPlugin:
#    remount: "true"
#    image:
#      registry: quay.io/
#      repository: openebs/jiva-csi
#      pullPolicy: IfNotPresent
#      tag: 3.4.0

cstor:

  # non csi configuration
  pool:
    image: "openebs/cstor-pool"
    imageTag: "2.12.2"
  poolMgmt:
    image: "openebs/cstor-pool-mgmt"
    imageTag: "2.12.2"
  target:
    image: "openebs/cstor-istgt"
    imageTag: "2.12.2"
  volumeMgmt:
    image: "openebs/cstor-volume-mgmt"
    imageTag: "2.12.2"

  # cstor csi driver configuration
  # do not enable or configure any sub dependency here
  # only cstor csi related settings can be added here
  # ref - https://openebs.github.io/cstor-operators

  # cstor chart dependency tree is here -
  # cstor
  # | - openebs-ndm

  # Enable openebs-ndm as root dependency not as sub dependency.
  # openebs
  # | - cstor(enable)
  # | | - openebs-ndm(disable)
  # | - openebs-ndm(enable)
  enabled: false
  openebsNDM:
    enabled: false

  # Sample configuration if you want to configure cstor csi driver with custom values.
  # This is a small part of the full configuration. Full configuration available
  # here - https://openebs.github.io/cstor-operators

#  imagePullSecrets: []
#
#  rbac:
#    create: true
#    pspEnabled: false
#
#  cspcOperator:
#    poolManager:
#      image:
#        registry: quay.io/
#        repository: openebs/cstor-pool-manager
#        tag: 3.4.0
#    cstorPool:
#      image:
#        registry: quay.io/
#        repository: openebs/cstor-pool
#        tag: 3.4.0
#    cstorPoolExporter:
#      image:
#        registry: quay.io/
#        repository: openebs/m-exporter
#        tag: 3.4.0
#    image:
#      registry: quay.io/
#      repository: openebs/cspc-operator
#      pullPolicy: IfNotPresent
#      tag: 3.4.0
#
#  cvcOperator:
#    target:
#      image:
#        registry: quay.io/
#        repository: openebs/cstor-istgt
#        tag: 3.4.0
#    volumeMgmt:
#      image:
#        registry: quay.io/
#        repository: openebs/cstor-volume-manager
#        tag: 3.4.0
#    volumeExporter:
#      image:
#        registry: quay.io/
#        repository: openebs/m-exporter
#        tag: 3.4.0
#    image:
#      registry: quay.io/
#      repository: openebs/cvc-operator
#      pullPolicy: IfNotPresent
#      tag: 3.4.0
#
#  cstorCSIPlugin:
#    image:
#      registry: quay.io/
#      repository: openebs/cstor-csi-driver
#      pullPolicy: IfNotPresent
#      tag: 3.4.0
#
#  admissionServer:
#    componentName: cstor-admission-webhook
#    image:
#      registry: quay.io/
#      repository: openebs/cstor-webhook
#      pullPolicy: IfNotPresent
#      tag: 3.4.0

# ndm configuration goes here
# https://openebs.github.io/node-disk-manager
openebs-ndm:
  enabled: false

  # Sample configuration if you want to configure openebs ndm with custom values.
  # This is a small part of the full configuration. Full configuration available
  # here - https://openebs.github.io/node-disk-manager

#  imagePullSecrets: []
#
#  ndm:
#    image:
#      registry: quay.io/
#      repository: openebs/node-disk-manager
#      pullPolicy: IfNotPresent
#      tag: 2.1.0
#    sparse:
#      path: "/var/openebs/sparse"
#      size: "10737418240"
#      count: "0"
#    filters:
#      enableOsDiskExcludeFilter: true
#      osDiskExcludePaths: "/,/etc/hosts,/boot"
#      enableVendorFilter: true
#      excludeVendors: "CLOUDBYT,OpenEBS"
#      enablePathFilter: true
#      includePaths: ""
#      excludePaths: "loop,fd0,sr0,/dev/ram,/dev/dm-,/dev/md,/dev/rbd,/dev/zd"
#    probes:
#      enableSeachest: false
#      enableUdevProbe: true
#      enableSmartProbe: true
#
#  ndmOperator:
#    image:
#      registry: quay.io/
#      repository: openebs/node-disk-operator
#      pullPolicy: IfNotPresent
#      tag: 2.1.0
#
#  helperPod:
#    image:
#      registry: quay.io/
#      repository: openebs/linux-utils
#      pullPolicy: IfNotPresent
#      tag: 3.4.0
#
#  featureGates:
#    enabled: true
#    GPTBasedUUID:
#      enabled: true
#      featureGateFlag: "GPTBasedUUID"
#    APIService:
#      enabled: false
#      featureGateFlag: "APIService"
#      address: "0.0.0.0:9115"
#    UseOSDisk:
#      enabled: false
#      featureGateFlag: "UseOSDisk"
#    ChangeDetection:
#      enabled: false
#      featureGateFlag: "ChangeDetection"
#
#  varDirectoryPath:
#    baseDir: "/var/openebs"

  # local pv provisioner configuration goes here
  # do not enable or configure any sub dependency here
  # ref - https://openebs.github.io/dynamic-localpv-provisioner

  # local pv chart dependency tree is here -
  # localpv-provisioner
  # | - openebs-ndm

  # Enable openebs-ndm as root dependency not as sub dependency.
  # openebs
  # | - localpv-provisioner(enable)
  # | | - openebs-ndm(disable)
  # | - openebs-ndm(enable)
localpv-provisioner:
  enabled: false
  openebsNDM:
    enabled: false

  # Sample configuration if you want to configure openebs locapv with custom values.
  # This is a small part of the full configuration. Full configuration available
  # here - https://openebs.github.io/dynamic-localpv-provisioner

#  imagePullSecrets: []
#
#  rbac:
#    create: true
#    pspEnabled: false
#
#  localpv:
#    image:
#      registry: quay.io/
#      repository: openebs/provisioner-localpv
#      tag: 3.4.0
#      pullPolicy: IfNotPresent
#    healthCheck:
#      initialDelaySeconds: 30
#      periodSeconds: 60
#    replicas: 1
#    enableLeaderElection: true
#    basePath: "/var/openebs/local"
#
#  helperPod:
#    image:
#      registry: quay.io/
#      repository: openebs/linux-utils
#      pullPolicy: IfNotPresent
#      tag: 3.4.0

# zfs local pv configuration goes here
# ref - https://openebs.github.io/zfs-localpv
zfs-localpv:
  enabled: false

  # Sample configuration if you want to configure zfs locapv with custom values.
  # This is a small part of the full configuration. Full configuration available
  # here - https://openebs.github.io/zfs-localpv

#  imagePullSecrets: []
#
#  rbac:
#    pspEnabled: false
#
#  zfsPlugin:
#    image:
#      registry: quay.io/
#      repository: openebs/zfs-driver
#      pullPolicy: IfNotPresent
#      tag: 2.1.0

# lvm local pv configuration goes here
# ref - https://openebs.github.io/lvm-localpv
lvm-localpv:
  enabled: false

  # Sample configuration if you want to configure lvm localpv with custom values.
  # This is a small part of the full configuration. Full configuration available
  # here - https://openebs.github.io/lvm-localpv

#  imagePullSecrets: []
#
#  rbac:
#    pspEnabled: false
#
#  lvmPlugin:
#    image:
#      registry: quay.io/
#      repository: openebs/lvm-driver
#      pullPolicy: IfNotPresent
#      tag: 1.0.0

# openebs nfs provisioner configuration goes here
# ref - https://openebs.github.io/dynamic-nfs-provisioner
nfs-provisioner:
  enabled: false

  # Sample configuration if you want to configure lvm localpv with custom values.
  # This is a small part of the full configuration. Full configuration available
  # here - https://openebs.github.io/dynamic-nfs-provisioner

#  imagePullSecrets: []
#
#  rbac:
#    pspEnabled: false
#
#  nfsProvisioner:
#    image:
#      registry:
#      repository: openebs/provisioner-nfs
#      tag: 0.10.0
#      pullPolicy: IfNotPresent
#    enableLeaderElection: "true"
#    nfsServerAlpineImage:
#      registry:
#      repository: openebs/nfs-server-alpine
#      tag: 0.10.0

cleanup:
  image:
    # Make sure that registry name end with a '/'.
    # For example : quay.io/ is a correct value here and quay.io is incorrect
    registry:
    repository: bitnami/kubectl
    tag:
    imagePullSecrets: []
      #  - name: image-pull-secret
